{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expose same api with gym \n",
    "# so I dont need to change the training code\n",
    "\n",
    "class Space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n-1)\n",
    "\n",
    "class WatchYourBack:\n",
    "    def __init__(self, original_map, states):\n",
    "        self.player_container_cells = [2, 5]\n",
    "        self.states = states\n",
    "        self.original_map = original_map\n",
    "        self.map = None\n",
    "        self.action_space = Space(4)\n",
    "        self.observation_space = Space(len(states))\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.map = deepcopy(self.original_map)\n",
    "        return self.states[self.encode_map(self.map)]\n",
    "        \n",
    "    def get_player_pos(self):\n",
    "        for j in range(len(self.map)):\n",
    "            for i in range(len(self.map[0])):\n",
    "                if self.map[j][i] in self.player_container_cells:\n",
    "                    return [j, i]\n",
    "\n",
    "    def render(self):\n",
    "        for row in self.map:\n",
    "            print(row)\n",
    "            \n",
    "    def encode_map(self, m):\n",
    "        ret = ''\n",
    "        for r in m:\n",
    "            ret = ret + ''.join([str(c) for c in r])\n",
    "        return ret\n",
    "    \n",
    "    def close(self):\n",
    "        return 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        # new_state, reward, done, info = env.step(action)\n",
    "        #\n",
    "        # 0: up, 1: right, 2: down, 3: left\n",
    "        \n",
    "        done = False\n",
    "        new_state = None\n",
    "        reward = 0\n",
    "        info = None\n",
    "        \n",
    "        player_movable_cells = [0, 3]\n",
    "        xy = {\n",
    "            0: [0, -1],\n",
    "            1: [1,  0],\n",
    "            2: [0,  1],\n",
    "            3: [-1, 0]\n",
    "        }\n",
    "        \n",
    "        [delta_x, delta_y] = xy[action]\n",
    "        [player_pos_y, player_pos_x] = self.get_player_pos()\n",
    "        [target_pos_y, target_pos_x] = [player_pos_y + delta_y, player_pos_x + delta_x]\n",
    "        \n",
    "        if target_pos_x >= 0 and target_pos_x < len(self.map[0]) and \\\n",
    "            target_pos_y >= 0 and target_pos_y < len(self.map) and \\\n",
    "            self.map[target_pos_y][target_pos_x] in player_movable_cells:\n",
    "\n",
    "            targetCellId = self.map[target_pos_y][target_pos_x]\n",
    "            self.map[player_pos_y][player_pos_x] = 0\n",
    "\n",
    "            if targetCellId == 3:\n",
    "                # hit the goal\n",
    "                reward = 1\n",
    "                done = True\n",
    "                self.map[target_pos_y][target_pos_x] = 5\n",
    "                info = \"hit the goal!\"\n",
    "            else:\n",
    "                self.map[target_pos_y][target_pos_x] = 2\n",
    "                info = f\"player moved to {target_pos_y} {target_pos_x}\"\n",
    "        \n",
    "        else:\n",
    "            info = \"player could not move\"\n",
    "            \n",
    "        # each state has a number value\n",
    "        new_state = self.states[self.encode_map(self.map)]\n",
    "        \n",
    "        return new_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_states_for_map(m):\n",
    "    ret = {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = {\n",
    "#     '203000000': 0,\n",
    "#     '023000000': 1,\n",
    "#     '005000000': 2, # goal state\n",
    "#     '003200000': 3,\n",
    "#     '003020000': 4,\n",
    "#     '003002000': 5,\n",
    "#     '003000200': 6,\n",
    "#     '003000020': 7,\n",
    "#     '003000002': 8\n",
    "# }\n",
    "# original_map = [\n",
    "#     [0, 0, 3],\n",
    "#     [0, 0, 0],\n",
    "#     [2, 0, 0]\n",
    "# ]\n",
    "\n",
    "states = {\n",
    "    '123010000': 0,\n",
    "    '105010000': 1, # goal\n",
    "    '103210000': 2,\n",
    "    '103012000': 3,\n",
    "    '103010200': 4,\n",
    "    '103010020': 5,\n",
    "    '103010002': 6,\n",
    "}\n",
    "\n",
    "original_map = [\n",
    "    [1, 0, 3],\n",
    "    [0, 1, 0],\n",
    "    [2, 0, 0]\n",
    "]\n",
    "\n",
    "env = WatchYourBack(original_map, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize q-table\n",
    "# \n",
    "# rows: state space in the environment\n",
    "# columns: action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters\n",
    "# \n",
    "# \n",
    "\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        # explore or exploit?\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])  # exploit: get highest q-value move\n",
    "        else:\n",
    "            action = env.action_space.sample()    # explore: select a random move\n",
    "        \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "        # Add new reward        \n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Average reward per thousand episodes********\n",
      "\n",
      "1000 :  0.9970000000000008\n",
      "2000 :  1.0000000000000007\n",
      "3000 :  1.0000000000000007\n",
      "4000 :  1.0000000000000007\n",
      "5000 :  1.0000000000000007\n",
      "6000 :  1.0000000000000007\n",
      "7000 :  1.0000000000000007\n",
      "8000 :  1.0000000000000007\n",
      "9000 :  1.0000000000000007\n",
      "10000 :  1.0000000000000007\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.95099005, 0.95099005, 0.96059601, 0.95099005],\n",
       "       [1.        , 0.99      , 0.9801    , 0.99      ],\n",
       "       [0.95099005, 0.970299  , 0.96059601, 0.96059601],\n",
       "       [0.970299  , 0.9801    , 0.970299  , 0.96059601],\n",
       "       [0.99      , 0.9801    , 0.9801    , 0.970299  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch it play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 5]\n",
      "[0, 1, 0]\n",
      "[0, 0, 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ebce0d1703e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Choose action with highest Q-value for current state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "        # Choose action with highest Q-value for current state       \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                # Agent reached the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode   \n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
