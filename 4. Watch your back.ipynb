{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMPTY_CELL = 0\n",
    "WALL = 1\n",
    "PLAYER_ON_EMPTY_CELL = 2\n",
    "PORTAL = 3\n",
    "PLAYER_ON_PORTAL = 5\n",
    "ENEMY_ANT = 6\n",
    "\n",
    "PLAYER_CONTAINER_CELLS = [PLAYER_ON_EMPTY_CELL, PLAYER_ON_PORTAL]\n",
    "PLAYER_MOVABLE_CELLS = [EMPTY_CELL, PORTAL]\n",
    "ENEMY_MOVABLE_CELLS = [EMPTY_CELL, PLAYER_ON_EMPTY_CELL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_map(m):\n",
    "    ret = ''\n",
    "    for r in m:\n",
    "        ret = ret + ''.join([str(c) for c in r])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_map(s, size):\n",
    "    ret = []\n",
    "    for j in range(size):\n",
    "        row = []\n",
    "        for i in range(size):\n",
    "            row.append(s[j*size + i])\n",
    "        ret.append(row)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_map(m):\n",
    "    for row in m:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expose same api with gym \n",
    "# so I dont need to change the training code\n",
    "\n",
    "class Space:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n-1)\n",
    "\n",
    "class WatchYourBack:\n",
    "    def __init__(self, original_map, states):\n",
    "        self.states = states\n",
    "        self.original_map = original_map\n",
    "        self.map = None\n",
    "        self.action_space = Space(4)\n",
    "        self.observation_space = Space(len(states))\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.map = deepcopy(self.original_map)\n",
    "        return self.states[encode_map(self.map)]\n",
    "        \n",
    "    def get_player_pos(self):\n",
    "        for j in range(len(self.map)):\n",
    "            for i in range(len(self.map[0])):\n",
    "                if self.map[j][i] in PLAYER_CONTAINER_CELLS:\n",
    "                    return [j, i]\n",
    "\n",
    "    def render(self):\n",
    "        print_map(self.map)\n",
    "                \n",
    "    def close(self):\n",
    "        return 0\n",
    "    \n",
    "    def move_enemies(self):\n",
    "        [player_pos_y, player_pos_x] = self.get_player_pos()\n",
    "    \n",
    "    def step(self, action):\n",
    "        # new_state, reward, done, info = env.step(action)\n",
    "        #\n",
    "        # 0: up, 1: right, 2: down, 3: left\n",
    "        \n",
    "        done = False\n",
    "        new_state = None\n",
    "        reward = 0\n",
    "        info = None\n",
    "        \n",
    "        xy = {\n",
    "            0: [0, -1],\n",
    "            1: [1,  0],\n",
    "            2: [0,  1],\n",
    "            3: [-1, 0]\n",
    "        }\n",
    "        \n",
    "        [delta_x, delta_y] = xy[action]\n",
    "        [player_pos_y, player_pos_x] = self.get_player_pos()\n",
    "        [target_pos_y, target_pos_x] = [player_pos_y + delta_y, player_pos_x + delta_x]\n",
    "        \n",
    "        if target_pos_x >= 0 and target_pos_x < len(self.map[0]) and \\\n",
    "            target_pos_y >= 0 and target_pos_y < len(self.map) and \\\n",
    "            self.map[target_pos_y][target_pos_x] in PLAYER_MOVABLE_CELLS:\n",
    "\n",
    "            targetCellId = self.map[target_pos_y][target_pos_x]\n",
    "            self.map[player_pos_y][player_pos_x] = 0\n",
    "\n",
    "            if targetCellId == 3:\n",
    "                # hit the goal\n",
    "                reward = 1\n",
    "                done = True\n",
    "                self.map[target_pos_y][target_pos_x] = 5\n",
    "                info = \"hit the goal!\"\n",
    "            else:\n",
    "                self.map[target_pos_y][target_pos_x] = 2\n",
    "                info = f\"player moved to {target_pos_y} {target_pos_x}\"\n",
    "        \n",
    "        else:\n",
    "            info = \"player could not move\"\n",
    "            \n",
    "        # each state has a number value\n",
    "        new_state = self.states[encode_map(self.map)]\n",
    "        \n",
    "        return new_state, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_states_for_playerless_map(m, enemy_count=0):\n",
    "    \n",
    "    # player movements\n",
    "    states = []\n",
    "    for j in range(len(m)):\n",
    "        for i in range(len(m[0])):\n",
    "            if m[j][i] == 0:\n",
    "                copy_m = deepcopy(m)\n",
    "                copy_m[j][i] = 2 # move player to there\n",
    "                states.append(encode_map(copy_m))\n",
    "        \n",
    "    # enemies \n",
    "    states_with_enemy = []    \n",
    "    for s in states:\n",
    "        for cellIndex in range(len(s)):\n",
    "            cell = int(s[cellIndex])\n",
    "            if cell == EMPTY_CELL:\n",
    "                copy_s = s[:cellIndex] + '6' + s[cellIndex+1:]\n",
    "                states_with_enemy.append(copy_s)\n",
    "\n",
    "    # player reaching the goal\n",
    "    states_with_goal = states_with_enemy[:]\n",
    "    for s in states_with_enemy:\n",
    "        for cellIndex in range(len(s)):\n",
    "            cell = int(s[cellIndex])\n",
    "            if cell == 3:\n",
    "                copy_s = s[:cellIndex] + '5' + s[cellIndex+1:]\n",
    "                copy_s = copy_s.replace('2', '0')\n",
    "                states_with_goal.append(copy_s)\n",
    "\n",
    "\n",
    "    dct = {}\n",
    "    for index in range(len(states_with_enemy)):\n",
    "        dct[states_with_enemy[index]] = index\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL STATES: 42\n"
     ]
    }
   ],
   "source": [
    "states = create_states_for_playerless_map([\n",
    "    [1, 0, 3],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0]\n",
    "], enemy_count = 1)\n",
    "\n",
    "print('TOTAL STATES:', len(states))\n",
    "\n",
    "original_map = [\n",
    "    [1, 0, 3],\n",
    "    [6, 0, 0],\n",
    "    [0, 0, 2]\n",
    "]\n",
    "\n",
    "env = WatchYourBack(original_map, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize q-table\n",
    "# \n",
    "# rows: state space in the environment\n",
    "# columns: action space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup hyperparameters\n",
    "# \n",
    "# \n",
    "\n",
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0 False player moved to 1 2\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'105600000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6ea4e6201a5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Take new action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-99a1a985aac2>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# each state has a number value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencode_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '105600000'"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode): \n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        \n",
    "        # explore or exploit?\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])  # exploit: get highest q-value move\n",
    "        else:\n",
    "            action = env.action_space.sample()    # explore: select a random move\n",
    "        \n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        print(new_state, reward, done, info)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "        \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "\n",
    "        # Add new reward        \n",
    "        rewards_current_episode += reward \n",
    "\n",
    "        if done == True: \n",
    "            break\n",
    "\n",
    "    # Exploration rate decay   \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "    \n",
    "    # Add current episode reward to total rewards list\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"********Average reward per thousand episodes********\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch it play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch our agent play Frozen Lake by playing the best action \n",
    "# from each state according to the Q-table\n",
    "\n",
    "for episode in range(3):\n",
    "    # initialize new episode params\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        # Show current state of environment on screen\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "    \n",
    "        # Choose action with highest Q-value for current state       \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "\n",
    "        # Take new action\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                # Agent reached the goal and won episode\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                # Agent stepped in a hole and lost episode   \n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            \n",
    "        # Set new state\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
